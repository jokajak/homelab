# 20231005

Today I tried to add 2 new controller nodes and I am pretty sure it killed my cluster.

The initial controller started shutting down and I noticed error messages about housekeeping not completing in time. So
my assumption is that the disks on the raspberry pi systems are too slow (even though they are USB SSD drives), maybe
due to too many writes due to logging to disk over USB. Then etcd says the system is unhealthy and shuts it down.

So I think I'm just going to use my Synology NAS to host an external database instead of using the embedded etcd.

## Setting up Synology NAS as postgres server

I followed the instructions on https://neellik.com/install-postgresql-11-on-your-synology-nas-with-docker/

I missed the part where the port was changed from 5432 to 5433 which complicated setting up the database.

I used the following docker-compose file:

```yaml
version: '3.9'
services:
  postgres:
    container_name: postgresql
    image: 'postgres:16'
    mem_limit: 256m
    cpu_shares: 768
    environment:
    - POSTGRES_PASSWORD=[Removed]
    - POSTGRES_USER=[Removed]
    - POSTGRES_DB=database
    ports:
    - '5433:5432'
    volumes:
    - '/volume2/nvme/postgresql:/var/lib/postgresql/data'
    healthcheck:
      test: ["CMD", "pg_isready", "-q", "-d", "database", "-U", "Zesty0089"]
    restart: on-failure:5
```

And I ran the following commands manually:

```shell
docker exec -it -u 999 postgresql /bin/bash
psql -U ${POSTGRES_USER} -d database
-- Create the database 'kubernetes'
CREATE DATABASE kubernetes;

-- Switch to the 'kubernetes' database
\c kubernetes

-- Create the user 'k3s' with a password
CREATE USER k3s WITH PASSWORD 'your_password_here';

-- Grant all privileges on the 'kubernetes' database to the 'k3s' user
GRANT ALL PRIVILEGES ON DATABASE kubernetes TO k3s;

-- Per https://stackoverflow.com/a/74111630
ALTER DATABASE kubernetes OWNER TO k3s;

-- Exit the PostgreSQL session
\q
```

On the bright side of making this change, I can have an even number of controller nodes.

I successfully deployed the cluster again (had to run within my container so that I had sops) but my main controller
powered off again. I'll have to investigate that.

Turns out it was due to overheating per:

```shell
Oct 05 15:47:21 controller kernel: thermal thermal_zone0: acpitz: critical temperature reached, shutting down
Oct 05 15:47:21 controller kernel: reboot: HARDWARE PROTECTION shutdown (Temperature too high)
```
